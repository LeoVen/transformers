# Geração de Código Python com Transformers

## Resumo

Este trabalho consiste no estudo e na aplicação de Redes Neurais no contexto de Processamento de Linguagens Naturais, mais especificamente na área de Geração de Linguagens Naturais, utilizando uma arquitetura de Transformers para a geração de código fonte na linguagem de programação Python. O objetivo foi reproduzir código Python a fim de explorar as capacidades do modelo GPT-2 (baseado em transformer) que pudessem auxiliar na programação e trazer novas ideias. Foi utilizado um modelo pré-treinado do GPT-2 e então realizado o fine-tuning em um conjunto de dados contendo código em Python. O modelo foi capaz de reproduzir códigos em Python simples, mas em alguns casos, contendo alguns erros de sintaxe ou lógica. Em exemplos mais complexos, o modelo não conseguiu reproduzir código relacionado ao código exemplo.
